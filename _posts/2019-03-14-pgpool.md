---
layout: post
title: Postgres automatic and (mostly) transparent) failover with pgpool
published: false
---

# Intro

I have been working a lot with pgpool recently and I wanted to document my experiments in case it is helpful for others. I deployed postgres and pgpool as docker images, either in a docker swarm or directly on the servers (i.e. using docker run to start the containers) and I build a graphical interface to operate and vizualize the cluster. This is all in the github repo <https://github.com/saule1508/pgcluster>.

But in this post I will not use docker, I will document how to install postgres/pgpool natively and how to configure pgpool to automate the failover. This said, if you start a learning journey with pgpool and postgres, I find it makes a lot of sense to use docker because it makes experimenting easier (you can quickly start from scratch). I will put emphasis on pgpool concepts or configurations that I found difficult to grasp (sometimes because they are poorly documented), and will simply list the steps to have postgres and repmgr set-up. 

I will build a set-up based on 3 Centos 7.6 servers: one primary database and two stand-by databases, one pgpool on each node made high-available with the watch-dog mode of pgpool. I am using libvirt to provision the VM (I provision one server then clone it), but you can use vagrant, virtual box, VMWare or provision the centos guests manually. If you have a linux host available then I suggest to use libvirt, I documented here a way to provision a centos guest from the command line in a few minutes: [Centos guest VM with cloud-init](http://saule1508.github.io/libvirt-centos-cloud-image/), this is just great.

# pgpool: some facts

I will go in details in pgpool installation and configuration later in this post, but here I will first do a short introduction. Pgpool is hard to learn because it can do a lot of things, but in most cases there are 3 main functions that are useful while the other are more esoteric or for historic reasons (for example pgpool can implement its own postgres replication mechanism or can do sql query result caching). Here are the main functions of pgpool:

## Connection caching

pgpool is not a connection pool, it is a connection cache. When pgpool starts it creates a number (determined by the parameter num_init_children) of child processes, initially all those child processes are in state "waiting for connection". When a client application connects to pgpool, one of the child process will be picked-up and establish a connection to postgres: so at this stage there are two processes for this client, the pgpool process and the postgres backend process. When the application disconnect from pgpool, the postgres backend process will remain. So that if another client connects to pgpool there is no need to establish a connection to postgres, the existing one can be re-used. The idea is that creating a connection to postgres is an expensive operation and that pgpool can decrease the load

But modern applications often use connection pooling, especially in the java world. Those connection pools re-use connections at the session level, i.e. one thread will grab a connection from the pool, perform some transactions then put the connection back in the pool (at that point the connection pooler will do a commit or a rollback to ensure that the session is clean, i.e. no locks or uncomitted changes). In this case the connection caching of pgpool might not be interesting and it can be disabled (parameter connection_cache='off')

## Automated and (mostly) transparent failover 

Postgres has a strong replication mechanisme (hot standby) in which a primary database streams its transaction logs (wal in postgres terminology, write-ahead-logs) and one or more standby databases receive those write-ahead-log and apply them continuously so that all changes done on the primary are replicated on the standbys. The primary database is open read-write, the stand-by databases are opened read-only. But postgres does not offer any automated way to promote a stand-by database in case of failure of the primary. Furthermore, if a stand-by database is manually promoted and become the new read-write database, all applications connected to the database must now be made aware that they must connect to a new instance.

pgpool is usually used on top of the postgres databases, acting as a proxy, it uses the following mechanisms:

* health check: pgpool uses a user (health_check_user) to connect regularly (health_check_period) to the databases and so is aware when a primary (or a standby fails). After a retry period (health_check_retry_interval and health_check_retries) it will trigger a failover, which concretely mean calling a script called with a number of arguments. 
* streaming replication check (sr_replication_check): pgpool will compute how far (in bytes) the standby is behind the primary and will stop using the standby if it lags too far behind (sr_replication_threshold)
  
Note that health checks is not the only mechanism that can be used to trigger the failover, there is also a paramter failover_on_backend_error (more on that later)

Transparent failover is achieved because clients are connecting to pgpool and not directly to postgres: if a failover happens, then pgpool knows that the primary database has changed and it will destroy then re-create all connections to postgres without any configuration needed on the client side. Of course it is not completly transparent, any statement (query or update) that was being executed will return an error.

But if pgpool is a proxy between the applications and postgres, a failure of pgpool itself would result in a lost of availability. In other words pgpool itself has become the single point of failure and of course it has a very nice mechanism that makes itself HA: the watchdog mode

## pgpool watchdog mode

in watchdog mode, a cluster of pgpool nodes (ideally 3) work together in active/passive mode: one node is elected the leader via a consensus algorithm. The leader acquires the VIP (virtual IP, also called delegate IP) and the leader takes responsability to perform the failover of postgres when needed. The pgpool nodes monitor themself via a heartbeat mechanism, should the master pgpool node fail then the cluster will elect a new leader and the VIP will be moved to the new pgpool node.




# Step 1: provision 3 guests machines with centos 7, postgres, repmgr and pgpool

The installation of the centos 7 guest is described in the link above, I will start from an existing server created from a centos base install.

It needs a file system /u01 for the database (the DB will be in /u01/pg11/data), a file system /u02 for the archived write ahead logs (wal) and the backups (/u02/archive and /u02/backup). I create a user postgres with uid 50010 (arbitrary id, does not matter but it is better to have the same id on all servers). The user must be created before the rpm is installed.

## Postgres

```bash
# as user root
MAJORVER=11
MINORVER=2
yum update -y
# install some useful packages
yum install -y epel-release libxslt sudo openssh-server openssh-clients jq passwd rsync iproute python-setuptools \
    hostname inotify-tools yum-utils which sudo vi firewalld
# create ssh keys pair
if [ ! -f /root/.ssh/id_rsa.pub ] ; then
  ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key -N ''
fi
# create user postgres before installing postgres rpm, because I want to fix the uid
useradd -u 50010 postgres
# set a password
passwd postgres
# So that postgres can become root without password, I add it in sudoers 
echo "postgres ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/postgres
# install postgres release rpm, this will add a file /etc/yum.repos.d/pgdg-11-centos.repo
yum install -y https://download.postgresql.org/pub/repos/yum/${MAJORVER}/redhat/rhel-7-x86_64/pgdg-centos${MAJORVER}-${MAJORVER}-${MINORVER}.noarch.rpm
# with the repo added (repo is called pgdg11), ww can install postgres
# NB: postgres is also available from Centos base repos, but we want the latest version (11)
yum install -y postgresql${MAJORVER}-${MAJORVER}.${MINORVER} postgresql${MAJORVER}-server-${MAJORVER}.${MINORVER}  postgresql${MAJORVER}-contrib-${MAJORVER}.${MINORVER}
# verify 
yum list installed postgresql*
```
I want to store the postgres database in /u01/pg11/data (non-default location), I want to have the archived wal in /u02/archive and the backup in /u02/backup
```bash
mkdir -p /u01/pg${MAJORVER}/data /u02/archive /u02/backup
chown postgres:postgres /u01/pg${MAJORVER}/data /u02/archive /u02/backup
chmod 700 /u01/pg${MAJORVER}/data /u02/archive
```
The environment variable PGDATA is important: it points to the database location. We need to change it from the rpm default (which is /var/lib/pgsql/11/data) to the new location /u01/pg11/data.

```bash
export PGDATA=/u01/pg${MAJORVER}/data
# add the binaries in the path of all users
echo "export PATH=\$PATH:/usr/pgsql-${MAJORVER}/bin" >  /etc/profile.d/postgres.sh
# source /etc/profile in bashrc of user postgres, make sure that PGDATA is defined and also PGVER so that we 
# can use PGVER in later scripts
echo "[ -f /etc/profile ] && source /etc/profile" >> /home/postgres/.bashrc 
echo "export PGDATA=/u01/pg${MAJORVER}/data" >> /home/postgres/.bashrc
echo "export PGVER=${MAJORVER}" >> /home/postgres/.bashrc
```
We need a systemd unit file for postgres, so that it is started automatically when we boot the server (see <https://www.postgresql.org/docs/11/server-start.html>)

```bash
cat <<EOF > /etc/systemd/system/postgresql.service
[Unit]
Description=PostgreSQL database server
Documentation=man:postgres(1)

[Service]
Type=notify
User=postgres
ExecStart=/usr/pgsql-11/bin/postgres -D /u01/pg11/data
ExecReload=/bin/kill -HUP $MAINPID
KillMode=mixed
KillSignal=SIGINT
TimeoutSec=0

[Install]
WantedBy=multi-user.target
EOF
```
enable the unit but don't start it yet

```bash
sudo systemctl enable postgresql
```
Now we can init the database, as user postgres

```bash
su - postgres
# check that PGDATA and the PATH are correct
echo $PGDATA
echo $PATH
pg_ctl -D ${PGDATA} initdb -o "--auth=trust --encoding=UTF8 --locale='en_US.UTF8'"
```
postgres configuration parameters are in $PGDATA/postgres.conf, I prefer not to change this file but include an additional file from the config.d directory and override some of the defaults parameter. I add the line "include_dir='conf.d'" at the end of postgresql.conf and then I add custom configurations in conf.d
```bash
# as user postgres
mkdir $PGDATA/conf.d
echo "include_dir = 'conf.d'" >> $PGDATA/postgresql.conf
# now let's add some config in this conf.d directory
cat <<EOF > $PGDATA/conf.d/custom.conf
log_destination = 'syslog,csvlog'
logging_collector = on
# better to put the logs outside PGDATA so they are not included in the base_backup
log_directory = '/var/log/postgres'
log_filename = 'postgresql-%Y-%m-%d.log'
log_truncate_on_rotation = on
log_rotation_age = 1d
log_rotation_size = 0
# These are relevant when logging to syslog (if wanted, change log_destination to 'csvlog,syslog')
log_min_duration_statement=-1
log_duration = on
log_line_prefix='%m %c %u - ' 
log_statement = 'all'
log_connections = on
log_disconnections = on
log_checkpoints = on
log_timezone = 'Europe/Brussels'
# up to 30% of RAM. Too high is not good.
shared_buffers = 512MB
#checkpoint at least every 15min
checkpoint_timeout = 15min 
#if possible, be more restrictive
listen_addresses='*'
#for standby
max_replication_slots = 5
archive_mode = on
archive_command = '/opt/postgres/scripts/archive.sh  %p %f'
# archive_command = '/bin/true'
wal_level = replica
max_wal_senders = 5
hot_standby = on
hot_standby_feedback = on
# for pg_rewind
wal_log_hints=true
EOF
```
I prefer to put the logs outside PGDATA, otherwise a backup might become very big just because of the logs. So we must create the directory

```bash
sudo mkdir /var/log/postgres
sudo chown postgres:postgres /var/log/postgres
```

As you can see I use the script /opt/postgres/scripts/archive.sh as archive command, this needs to be created

```bash
sudo mkdir -p /opt/postgres/scripts
sudo chown -R postgres:postgres /opt/postgres
```

```bash
# content of script /opt/postgres/scripts/archive.sh
cat <<EOF > /opt/postgres/scripts/archive.sh
LOGFILE=/var/log/postgres/archive.log
if [ ! -f $LOGFILE ] ; then
 touch $LOGFILE
fi
echo "archiving $1 to /u02/archive/$2"
cp $1 /u02/archive/$2
exit $?
EOF
chmod +x /opt/postgres/scripts/archive.sh
```

Now let's create a database and a user
```bash
sudo systemctl start posgresql
# verify
sudo systemctl status posgresql


psql -U postgres postgres
```
at the psql prompt, create a database (my database is called critlib), then quit
```
create database critlib encoding 'UTF8'  LC_COLLATE='en_US.UTF8';
\q
```
now reconnect to the newly create database and create two users (cl_owner and cl_user)
```
psql -U postgres critlib
create user cl_owner nosuperuser nocreatedb login password 'cl_owner';
create schema cl_owner authorization cl_owner;
create user cl_user nosuperuser nocreatedb login password 'cl_user';
grant usage on schema cl_owner to cl_user;
alter default privileges in schema cl_owner grant select,insert,update,delete on tables to cl_user;
alter role cl_user set search_path to "$user",cl_owner,public;
\q
```

Before cloning the server (we need 3 servers) I will install the packages repmgr and pgpool. After that we can clone the server (it is a VM !), set-up the streaming replication with repmgr and then configure pgpool

## install repmgr

Repmgr is a nice open-source tool made by 2ndquadrant. It is not mandatory to use it and at first it maybe worth to set-up the streaming replication using standard postgres commands, for learning purpose. But otherwise it is really worth using repmgr because it brings very well tested and documented scripts. Note that repmgr can also be used to automate the failover (via the repmgrd daemon process) but we don't want that because we will use pgpool.

```bash
MAJORVER=11
REPMGRVER=4.2
curl https://dl.2ndquadrant.com/default/release/get/${MAJORVER}/rpm | bash
yum install -y --enablerepo=2ndquadrant-dl-default-release-pg${MAJORVER} --disablerepo=pgdg${MAJORVER} repmgr${MAJORVER}-${REPMGRVER}
mkdir /var/log/repmgr && chown postgres:postgres /var/log/repmgr
```
set ownership of /etc/repmgr to postgres
```bash
chown -R postgres:postgres /etc/repmgr
```

## install pgpool

Adding pgpool on Centos is easy: install the pgpool release rpm (it will set-up a repo file in /etc/yum.repo.d) and then install pgpool itself via yum. 

```bash
export PGPOOLMAJOR=4.0
export PGPOOLVER=4.0.3
export PGVER=11

yum install -y http://www.pgpool.net/yum/rpms/${PGPOOLMAJOR}/redhat/rhel-7-x86_64/pgpool-II-release-${PGPOOLMAJOR}-1.noarch.rpm
yum install --disablerepo=pgdg11 --enablerepo=pgpool40 -y pgpool-II-pg11-${PGPOOLVER} pgpool-II-pg11-extensions-${PGPOOLVER} pgpool-II-pg11-debuginfo-${PGPOOLVER}
``

I prefer to have pgpool running as user postgres, so I will override the systemd unit file that was installed by the rpm

```bash
# as user root
cat <<EOF > /etc/systemd/system/pgpool.service.d/override.conf
[Service]
User=postgres
Group=postgres
EOF
```

Since pgpool connects to the various postgres servers via ssh, I add a few config to the ssh client to make ssh connections easier
```bash
cat <<EOF > /etc/ssh/ssh_config
StrictHostKeyChecking no 
UserKnownHostsFile /dev/null 
EOF
```
Since I will use postgres user to start pgpool, I will change the ownership of /opt/pgpool-II 

```bash
sudo chown postgres:postgres -R /etc/pgpool-II
```

I also need to create a directory for the pid file and the socket file
```
sudo mkdir /var/run/pgpool
sudo chown postgres:postgres /var/run/pgpool
```

## Clone the servers

Now that I have pg01 ready, I will clone it to pg02 and pg03

I am using virsh (KVM) to manage my guests, those are the commands I used to clone my VM (the VM was created with this procedure <http://saule1508.github.io/libvirt-centos-cloud-image>)

Note: this is mostly for my own record, your way of cloning a VM might be different of course or you might have physical servers or you might use ansible to automate the provisioning,...

```bash
# on my host
virsh shutdown pg01
sudo mkdir /u01/virt/{pg02,pg03}
sudo chown pierre:pierre /u01/virt/pg02 /u01/virt/pg03
virt-clone -o pg01 -n pg02 --file /u01/virt/pg02/pg02.qcow2 --file /u01/virt/pg02/pg02-disk1.qcow2
```
Now start the VM, get into it and change its IP address. To have a DNS, I also add an entry in /etc/hosts of the KVM host 
```
# /etc/hosts of the KVM server, add one entry per vm so that they can speak to each other via dns
# added
192.168.122.10 pg01.localnet
192.168.122.11 pg02.localnet
192.168.122.13 pg03.localnet
```
when doing so, one must stop and start the default network then restart libvirtd

```bash
virsh net-destroy default
virsh net-start default
sudo systemctl restart libvirtd
```

# Set-up streaming replication

Now I have 3 centos 7 VM with postgres, repmgr and pgpool installed. The 3 VM can resolve each other names via DNS but alternatively we could put 3 entries in the /etc/hosts file on each VM
| host          | IP             |     |
| ------------- | -------------- | --- |
| pg01.localnet | 192.168.122.10 |
| pg02.localnet | 192.168.122.11 |
| pg03.localnet | 192.168.122.12 |

We will use repmgr to set-up the 3 postgres instances in a primary - standby configuration where pg01 will be the primary. pg01 will be streaming his write ahead logs (wal) to pg02 and pg03.

First we want to set-up ssh keys so that each servers can connect to each other with the user postgres. On each server, generate a ssh keys pairt for user postgres and transfer the public key to both other servers

```bash 
# on pg01 as user postgres. Keep the default (no passphrase)
ss-keygen -t rsa
ssh-copy-id postgres@pg02.localnet
ssh-copy-id postgres@pg03.localnet
```
do the same on server pg02 and on server pg03

## primary database on pg01

For the streaming replication we will be using the user repmgr with password rep123. Let's create it.

create the user repmgr
```bash
# on pg01
psql <<-EOF
  create user repmgr with superuser login password 'rep123' ;
  alter user repmgr set search_path to repmgr,"\$user",public;
  \q
EOF
create a database called repmgr
```bash
# on pg01
psql --command "create database repmgr with owner=repmgr ENCODING='UTF8' LC_COLLATE='en_US.UTF8';"
```

We want to connect with repmgr without password, that's what the pgpass hidden file is for.

```bash
# on pg01
echo "*:*:repmgr:repmgr:rep123" > /home/postgres/.pgpass
echo "*:*:replication:repmgr:rep123" >> /home/postgres/.pgpass
chmod 600 /home/postgres/.pgpass
scp /home/postgres/.pgpass pg02.localnet:/home/postgres/.pgpass 
scp /home/postgres/.pgpass pg03.localnet:/home/postgres/.pgpass 
```

Add entries in $PGDATA/pg_hba.conf for repmgr
```bash
# on pg01
cat <<EOF >> $PGDATA/pg_hba.conf
# replication manager
local  replication   repmgr                      trust
host   replication   repmgr      127.0.0.1/32    trust
host   replication   repmgr      0.0.0.0/0       md5
local   repmgr        repmgr                     trust
host    repmgr        repmgr      127.0.0.1/32   trust
host    repmgr        repmgr      127.0.0.1/32   trust
host    repmgr        repmgr      0.0.0.0/0      md5
host    all           all         0.0.0.0/0      md5
EOF
sudo systemctl restart postgresql
```

create the repmgr configuration file
```bash
# on pg01
cat <<EOF > /etc/repmgr/11/repmgr.conf
node_id=1
node_name=pg01.localnet
conninfo='host=pg01.localnet dbname=repmgr user=repmgr password=rep123 connect_timeout=2'
data_directory='/u01/pg11/data'
use_replication_slots=yes
# event_notification_command='/opt/postgres/scripts/repmgrd_event.sh %n "%e" %s "%t" "%d" %p %c %a'
reconnect_attempts=10
reconnect_interval=1

restore_command = 'cp /u02/archive/%f %p'

log_facility=STDERR
failover=manual
monitor_interval_secs=5

pg_bindir='/usr/pgsql-11/bin'

service_start_command = 'sudo systemctl start postgresql'
service_stop_command = 'sudo systemctl stop postgresql'
service_restart_command = 'sudo systemctl restart postgresql'
service_reload_command = 'pg_ctl reload'

promote_command='repmgr -f /etc/repmgr/11/repmgr.conf standby promote'
follow_command='repmgr -f /etc/repmgr/11/repmgr.conf standby follow -W --upstream-node-id=%n'
EOF
```

Now register this database as being the primary database
```bash
# on pg01
repmgr -f /etc/repmgr/11/repmgr.conf -v master register
```
we can also enable the systemd unit postgresql, so that postgres will be started on next boot
```bash
# on pg01
sudo systemctl enable postgresql
```

## Standby database on pg02

Now we can go to the first standby, pg02, and set-it up as a standby 
```bash
# on pg02
cat <<EOF > /etc/repmgr/11/repmgr.conf
node_id=2
node_name=pg02.localnet
conninfo='host=pg02.localnet dbname=repmgr user=repmgr password=rep123 connect_timeout=2'
data_directory='/u01/pg11/data'
use_replication_slots=yes
# event_notification_command='/opt/postgres/scripts/repmgrd_event.sh %n "%e" %s "%t" "%d" %p %c %a'
reconnect_attempts=10
reconnect_interval=1

restore_command = 'cp /u02/archive/%f %p'

log_facility=STDERR
failover=manual
monitor_interval_secs=5

pg_bindir='/usr/pgsql-11/bin'

service_start_command = 'sudo systemctl start postgresql'
service_stop_command = 'sudo systemctl stop postgresql'
service_restart_command = 'sudo systemctl restart postgresql'
service_reload_command = 'pg_ctl reload'

promote_command='repmgr -f /etc/repmgr/11/repmgr.conf standby promote'
follow_command='repmgr -f /etc/repmgr/11/repmgr.conf standby follow -W --upstream-node-id=%n'
EOF
```

let's wipe out the database and the archived wal
```bash
# make sure postgres is not running !
sudo systemctl stop postgres
# on pg02 as user postgres
rm -rf /u01/pg11/data/*
rm -rf /u02/archive/*
```
And now let's set-up this server as a standby
```bash
repmgr -h pg01.localnet -U repmgr -d repmgr -D /u01/pg11/data -f /etc/repmgr/11/repmgr.conf standby clone
sudo systemctl start postgresql
repmgr -f /etc/repmgr/11/repmgr.conf standby register --force
```
we can enable the unit postgresql so that postgres will start automatically on next reboot

```bash
sudo systemctl enable postgresql
```
## Standby database on pg03

Let us do the same on the third standby, pg03, and set-it up as a standby 
```bash
# on pg03 as user postgres
cat <<EOF > /etc/repmgr/11/repmgr.conf
node_id=3
node_name=pg03.localnet
conninfo='host=pg03.localnet dbname=repmgr user=repmgr password=rep123 connect_timeout=2'
data_directory='/u01/pg11/data'
use_replication_slots=yes
# event_notification_command='/opt/postgres/scripts/repmgrd_event.sh %n "%e" %s "%t" "%d" %p %c %a'
reconnect_attempts=10
reconnect_interval=1

restore_command = 'cp /u02/archive/%f %p'

log_facility=STDERR
failover=manual
monitor_interval_secs=5

pg_bindir='/usr/pgsql-11/bin'

service_start_command = 'sudo systemctl start postgresql'
service_stop_command = 'sudo systemctl stop postgresql'
service_restart_command = 'sudo systemctl restart postgresql'
service_reload_command = 'pg_ctl reload'

promote_command='repmgr -f /etc/repmgr/11/repmgr.conf standby promote'
follow_command='repmgr -f /etc/repmgr/11/repmgr.conf standby follow -W --upstream-node-id=%n'
EOF
```

let's wipe out the database and the archived wal
```bash
# make sure postgres is not running !
sudo systemctl stop postgres
# on pg02 as user postgres
rm -rf /u01/pg11/data/*
rm -rf /u02/archive/*
```
And now let's set-up this server as a standby
```bash
repmgr -h pg01.localnet -U repmgr -d repmgr -D /u01/pg11/data -f /etc/repmgr/11/repmgr.conf standby clone
sudo systemctl start postgresql
repmgr -f /etc/repmgr/11/repmgr.conf standby register --force
```
we can enable the unit postgresql so that postgres will start automatically on next reboot

```bash
sudo systemctl enable postgresql
```

## Test streaming replication

```bash
# on pg01 as user postgres
# check the nodes table, it contains meta-data for repmgr 
psql -U repmgr repmgr -c "select * from nodes;"
# create a test table
psql -U rempgr repmgr -c "create table test(c1 int, message varchar(120)); insert into test values(1,'this is a test');"
# check on pg02 if I can see the table and if it is read-only
psql -h pg02.localnet -U repmgr repmgr <<EOF
select * from test;
drop table test;
EOF
# check on pg03 if I can see the table and if it is read-only
psql -h pg03.localnet -U repmgr repmgr <<EOF
select * from test;
drop table test;
EOF
# drop the table on pg01
psql -U repmgr repmgr -c "drop table test;"
```

## Configure pgpool

Now it is time to configure pgpool on our 3 servers. pgpool can be used as a connection cache but - more importantly - it is used to automate the failover of postgres and to make the failover (almost) transparent to client applications.

Remember that postgres is made HA (High Available) via the streaming replication: one primary database is open read-write and two standbys are open read only, continuously applying the wal (write ahead logs) streamed by the primary. If the primary fails then one of the stand-by needs to be opened read-write (becomes the new primary) but also the client applications must now connect to the new primary. That's why we need pgpool: clients application are connecting to pgpool that acts as a proxy between postgres.

But then of course pgpool itself must be made High Available: this is done via a Virtual IP (VIP) which is moved on the pgpool master node via a mechanism called watchdog.


The following line muse be added to the postgres config (not sure why and if it is still needed)

```bash
echo "pgpool.pg_ctl='/usr/pgsql-11/bin/pg_ctl'" >> $PGDATA/conf.d/custom.conf
```

### pgool_hba and pool_passwd files

similar to the postgres host based authentification mechanism (pg_hba), pgpool has a pool_hba.conf file. 

```bash
echo <<EOF >> /etc/pgpool-II/pool_hba.conf
local   all         all                               trust
# IPv4 local connections:
host     all         all         0.0.0.0/0             md5
EOF
scp /etc/pgpool-II/pool_hba.conf postgres@pg02.localnet:/etc/pgpool-II/
scp /etc/pgpool-II/pool_hba.conf postgres@pg03.localnet:/etc/pgpool-II/
```
Because I use md5, I will need to have the file pool_passwd containing the md5 hashed password <http://www.pgpool.net/docs/latest/en/html/runtime-config-connection.html#GUC-POOL-PASSWD>

```bash
# first dump the info into a temp file
psql -c "select rolname,rolpassword from pg_authid;" > /tmp/users.tmp
# then go through the file to remove/add the entry in pool_passwd file
cat /tmp/users.tmp | awk 'BEGIN {FS="|"}{print $1" "$2}' | grep md5 | while read f1 f2
do
 echo "setting passwd of $f1 in /etc/pgpool-II/pool_passwd"
 # delete the line if exits
 sed -i -e "/^${f1}:/d" /etc/pgpool-II/pool_passwd
 echo $f1:$f2 >> /etc/pgpool-II/pool_passwd
done
scp /etc/pgpool-II/pool_passwd pg02:/etc/pgpool-II/pool_passwd
scp /etc/pgpool-II/pool_passwd pg03:/etc/pgpool-II/pool_passwd
```

### Setting up pcp.conf file

pcp (pgpool control protocol) is an administrative interface to pgpool, it enables you to interact with pgpool via port 9898 (default). The file pcp.conf stores a username and a md5 password to authentificate, the file .pcppass enables a user to use pcp commands without password. Note that this user/password is not related to a postgres user, it is just a pcp user that can speak with pgpool via the pcp protocol. <http://www.pgpool.net/docs/latest/en/html/configuring-pcp-conf.html>

I will use the user postgres with password secret

```bash
echo "postgres:$(pg_md5 secret)" >> /etc/pgpool-II/pcp.conf
echo "*:*:postgres:$(pg_md5 secret)" > /home/postgres/.pcppass 
chown postgres:postgres /home/postgres/.pcppass 
chmod 600 /home/postgres/.pcppass
scp /home/postgres/.pcppass pg02:/home/postgres/.pcppass
scp /home/postgres/.pcppass pg03:/home/postgres/.pcppass
```
because of this .pcppass we will be able to use pcp without password when logged in as unix user postgres.

In watchdog mode, pgpool will need to execute the ip and the arping commands with user postgres, so we set the sticky bit on those two utilities.

```bash
chmod 4755 /usr/sbin/ip /usr/sbin/arping
```

### Pgpool configuration

I start from a sample file, /etc/pgpool-II/pgpool.conf.sample-stream, then I will override some value by adding them to the end of the file

The 3 servers used in the config are pg01.localnet, pg02.localnet and pg03.localnet. We also need a VIP (virtual IP, also called delegate_ip in the context of pgpool). In my case the VIP will be 192.168.122.50.


```bash
# as user postgres on pg01
CONFIG_FILE=/etc/pgpool-II/pgpool.conf
cp /etc/pgpool-II/pgpool.conf.sample-stream $CONFIG_FILE
cat <<EOF >> $CONFIG_FILE
# override some values

listen_addresses = '*'
port = 9999
socket_dir = '/var/run/pgpool'
pcp_listen_addresses = '*'
pcp_port = 9898
pcp_socket_dir = '/var/run/pgpool'
listen_backlog_multiplier = 2
serialize_accept = off
enable_pool_hba = on
pool_passwd = 'pool_passwd'
authentication_timeout = 60
ssl = off
num_init_children = 10
max_pool = 5
# - Life time -
child_life_time = 300
child_max_connections = 0
connection_life_time = 600
client_idle_limit = 0

syslog_facility = 'LOCAL0'
syslog_ident = 'pgpool'
debug_level = 0

pid_file_name = '/var/run/pgpool/pgpool.pid'
logdir = '/tmp'

connection_cache = on
reset_query_list = 'ABORT; DISCARD ALL'
#reset_query_list = 'ABORT; RESET ALL; SET SESSION AUTHORIZATION DEFAULT'
replication_mode = off
load_balance_mode = off
master_slave_mode = on
master_slave_sub_mode = 'stream'

backend_hostname0 = 'pg01.localnet'
backend_port0 = 5432
backend_weight0 = 1
backend_data_directory0 = '/u01/pg10/data'
backend_flag0 = 'ALLOW_TO_FAILOVER'

backend_hostname1 = 'pg02.localnet'
backend_port1 = 5432
backend_weight1 = 1
backend_data_directory1 = '/u01/pg10/data'
backend_flag1 = 'ALLOW_TO_FAILOVER'

backend_hostname2 = 'pg03.localnet'
backend_port2 = 5432
backend_weight2 = 1
backend_data_directory2 = '/u01/pg10/data'
backend_flag2 = 'ALLOW_TO_FAILOVER'


# this is about checking the streaming replication, i.e. what is the delay
sr_check_period = 10
sr_check_user = 'repmgr'
sr_check_password = 'rep123'
sr_check_database = 'repmgr'
delay_threshold = 10000000

# this is about automatic failover
failover_command = '/opt/pgpool/scripts/failover.sh  %d %h %P %m %H %R'
failback_command = 'echo failback %d %h %p %D %m %H %M %P'
failover_on_backend_error = 'off'
search_primary_node_timeout = 300
follow_master_command = '/opt/pgpool/scripts/follow_master.sh %d %h %m %p %H %M %P'

health_check_period = 40
health_check_timeout = 10
health_check_user = 'hcuser'
health_check_password = 'hcuser'
health_check_database = 'postgres'
health_check_max_retries = 3
health_check_retry_delay = 1
connect_timeout = 10000

#------------------------------------------------------------------------------
# ONLINE RECOVERY
#------------------------------------------------------------------------------
recovery_user = 'postgres'
recovery_password = 'postgres'
recovery_1st_stage_command = 'pgpool_recovery.sh'
recovery_2nd_stage_command = 'echo recovery_2nd_stage_command'
recovery_timeout = 90
client_idle_limit_in_recovery = 0

#------------------------------------------------------------------------------
# WATCHDOG
#------------------------------------------------------------------------------
use_watchdog = on
# trusted_servers = 'www.google.com,pg02,pg03' (not needed with a 3 nodes cluster)
ping_path = '/bin'
wd_hostname = pg01
wd_port = 9000
wd_priority = 1
wd_authkey = ''
wd_ipc_socket_dir = '/var/run/pgpool'
delegate_IP = 192.168.122.99
if_cmd_path = '/opt/pgpool/scripts'
if_up_cmd = 'ip_w.sh addr add $_IP_$/24 dev eth0 label eth0:0'
if_down_cmd = 'ip_w.sh addr del $_IP_$/24 dev eth0'
arping_path = '/opt/pgpool/scripts'
arping_cmd = 'arping_w.sh -U $_IP_$ -I eth0 -w 1'
# - Behaivor on escalation Setting -

heartbeat_destination0 = 'pg01.localnet'
heartbeat_destination_port0 = 9694
heartbeat_destination1 = 'pg02.localnet'
heartbeat_destination_port1 = 9694
heartbeat_destination2 = 'pg03.localnet'
heartbeat_destination_port2 = 9694

other_pgpool_hostname0 = 'pg02.localnet'
other_pgpool_port0 = 9999
other_wd_port0 = 9000

other_pgpool_hostname1 = 'pg03.localnet'
other_pgpool_port1 = 9999
other_wd_port1 = 9000
EOF
```

Some important parameters: 

* **params related to streaming replication.**

I prefer to set load_balance to 'off' but it might be a useful feature in some cases. By setting master_slave_mode on and sub mode to stream we are telling pgpool that we use standard postgres streaming replication mechanism.

```
load_balance_mode = off
master_slave_mode = on
master_slave_sub_mode = 'stream'
```

For each of the 3 postgres instance we need a backend section describing the backend

```
backend_hostname0 = 'pg01.localnet'
backend_port0 = 5432
backend_weight0 = 1
backend_data_directory0 = '/u01/pg10/data'
backend_flag0 = 'ALLOW_TO_FAILOVER'
etc... for pg02 and pg03
```

* **num_init_children and max_pool relate to the connection caching functionality of pgpool.** 

When pgpool starts it will create "num_init_children" client processes that will all be in a state "waiting for a connection". When a client application connects to pgpool, pgpool will use one of the child process that is free and establish a connection to postgres. When the client disconnects then pgpool will keep the connection to postgres open (connection caching) so that next time a client connect to pgpool the connection to postgres can be re-used and we avoid the cost of creating a new connection.

* **parameters related to automatic failover**

**failover_command** is the script that will be executed when pgpool detects (via health check mechanism in my case) that the primary database is down. I don't know what the **failback_command** is supposed to do, I just don't use it. The **follow_master_command** is executed on all standby nodes after the failover_command was executed and a new primary database has been detected. I wil show the corresponding scripts later in this document.

```
failover_command = '/opt/pgpool/scripts/failover.sh  %d %h %P %m %H %R'
failback_command = 'echo failback %d %h %p %D %m %H %M %P'
search_primary_node_timeout = 300
follow_master_command = '/opt/pgpool/scripts/follow_master.sh %d %h %m %p %H %M %P'
```
The first reason why people need pgpool is because pgpool can automate the failover and make it transparent to client applications. There are basically two mechanisms to trigger a failover. Either via failover_on_backend_error or via the health checks. I prefer to set failover_on_backend_error to off and to use the health check mechanism to trigger the failover. The health check mechanism means that pgpool will regularly connect to postgres, if a connection fails (either to a primary or to a standby) then after the number of retries it will trigger the failover. 

```
failover_on_backend_error = 'off'
health_check_period = 40
health_check_timeout = 10
health_check_user = 'hcuser'
health_check_password = 'hcuser'
health_check_database = 'postgres'
health_check_max_retries = 3
health_check_retry_delay = 1
```

If you set failover_on_backend_error to 'on', then pgpool will trigger the failover when one of the child process detects that postgres is terminated. In this case there is no retry. Note also that in this case, as long as no application connect to pgpool or perform a query on the database, no failover will be triggered.

Do not confuse the health check with the streaming replication check (the streaming replication check look at the delay in replication, if the delay is too big a standby will ne be used for read load balancing and will not be considered for promotion). For some strange reason pgpool uses two different users for these. 

* **parameters related to watch-dog**

The watch-dog mechanism is a very cool functionality of pgpool: multiple instances of pgpool (3 in our case) are in an active-passive set-up, on startup the cluster elects one leader and the leader acquires the VIP (delegate_ip). The 3 nodes are monitoring each other and if the primary fails then a new leader will be elected and the VIP will be moved to this new leader.

Only the leader pgpool node will execute the failover, follow_master, etc. commands

The watchdog mode must be enabled via use_watchdog ('on')

The nodes monitor each other via the heartbeat mechanism, sending a packet on port 9694 to the other nodes

```
heartbeat_destination0 = 'pg01.localnet'
heartbeat_destination_port0 = 9694
heartbeat_destination1 = 'pg02.localnet'
heartbeat_destination_port1 = 9694
heartbeat_destination2 = 'pg03.localnet'
heartbeat_destination_port2 = 9694
```

The VIP is acquired via script, we'll have to create the script on the hosts.
```
if_cmd_path = '/opt/pgpool/scripts'
if_up_cmd = 'ip_w.sh addr add $_IP_$/24 dev eth0 label eth0:0'
if_down_cmd = 'ip_w.sh addr del $_IP_$/24 dev eth0'
arping_path = '/opt/pgpool/scripts'
arping_cmd = 'arping_w.sh -U $_IP_$ -I eth0 -w 1'
```
In the command above, pgpool will replace $_IP_$ with the value of the delegate_ip parameter

```
delegate_ip = 192.168.122.99
```

each node must be aware of the others, so on pg01

```
wd_hostname = pg01
wd_port = 9000
wd_priority = 1
wd_authkey = ''
wd_ipc_socket_dir = '/var/run/pgpool'

other_pgpool_hostname0 = 'pg02.localnet'
other_pgpool_port0 = 9999
other_wd_port0 = 9000

other_pgpool_hostname1 = 'pg03.localnet'
other_pgpool_port1 = 9999
other_wd_port1 = 9000
```

### pgpool preparation

we need to create the postgres user for the health check
```bash
# on pg01 as user postgres
 psql -c "create user hcuser with login password 'hcuser';"
```

We need to open a few firewall ports, for pgpool (9999 and 9898 for pcp) and for the watch-dog functionality
```bash
# on all 3 servers
# ports for pgpool and for pcp
sudo firewall-cmd --add-port 9898/tcp --permanent
sudo firewall-cmd --add-port 9999/tcp --permanent

```

let's install our scripts




Before starting pgpool a bit more preparation is required

```bash
sudo systemctl restart firewalld
# directory for pid file
sudo chown postgres:postgres /var/run/pgpool
```

```bash
cat <<EOF >> $CONFIG_FILE




