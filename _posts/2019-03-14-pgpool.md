---
layout: post
title: Postgres automatic (and mostly transparent) failover with pgpool
published: false
---

I have been working a lot with pgpool recently and I wanted to document my experiments in case it is helpful for others. I deployed postgres and pgpool as docker images and I build a graphical interface to operate and vizualize the cluster. This is all in the github repo <https://github.com/saule1508/pgcluster>

However in this post I will rather install postgres/pgpool natively and explain the key pgpool concepts that I found difficult to grasp. This said, if you start a learning journey with pgpool and postgres, I find it makes a lot of sense to use docker as this makes experimenting much easier.

In this post I will document a 3 nodes clustes in which pgpool is made high-available via the watch-dog mode. This requires 3 servers. I am using Centos VM's provisioned automatically via libvirt, but you can use vagrant or provision the centos guests manually. If you have a linux host then I suggest to use libvirt, I documented here a way to provision a centos guest from the command line in a few minutes: [Centos guest VM with cloud-init](http://saule1508.github.io/libvirt-centos-cloud-image/), this is just great.

# Step 1: provision 3 guests machines with centos 7, postgres, repmgr and pgpool

Create a first Centos guest machine (use the link above if you have libvirt)

I create a directory /u01 in which the database will reside. I also create a user postgres with uid 50010 (arbitrary id, does not matter but it is better to have the same id on all servers)

## Postgres


```bash
# as user root
MAJORVER=11
MINORVER=2
PGPOOLMAJOR=4.0
PGPOOLVER=4.0.3
REPMGRVER=4.2
yum update -y
# install some useful packages
yum install -y epel-release libxslt sudo openssh-server openssh-clients jq passwd rsync iproute python-setuptools \
    hostname inotify-tools yum-utils which sudo vi
if [ ! -f /root/.ssh/id_rsa.pub ] ; then
  ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key -N ''
fi
# create user postgres before installing postgres rpm, because I want to fix the uid
useradd -u 50010 postgres
# set a password
passwd postgres
# because I like that postgres can become root without password, I add it in sudoers 
echo "postgres ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/postgres
# install postgres release rpm, this will add a file /etc/yum.repos.d/pgdg-11-centos.repo
yum install -y https://download.postgresql.org/pub/repos/yum/${MAJORVER}/redhat/rhel-7-x86_64/pgdg-centos${MAJORVER}-${MAJORVER}-${MINORVER}.noarch.rpm
# with the new repo defined (pgdg11), ww can install postgres
# of course postgres is also available from Centos base repos, but we want the latest version (11)
yum install -y postgresql${MAJORVER}-${MAJORVER}.${MINORVER} postgresql${MAJORVER}-server-${MAJORVER}.${MINORVER}  postgresql${MAJORVER}-contrib-${MAJORVER}.${MINORVER}
# verify 
yum list installed postgresql*
```
I want to store the postgres database in /u01/pg11/data (non-default location), I want to have the archived wal in /u02/archive and the backup in /u02/backup

```bash
mkdir -p /u01/pg${MAJORVER}/data /u02/archive /u02/backup
chown postgres:postgres /u01/pg${MAJORVER}/data /u02/archive /u02/backup
chmod 700 /u01/pg${MAJORVER}/data /u02/archive
```
The environment variable PGDATA is critical: it points to the database location. We need to change it from the rpm default (which is /var/lib/pgsql/11/data) to the new location /u01/pg11/data.

```
export PGDATA=/u01/pg${MAJORVER}/data
# add the binaries in the path of all users
echo "export PATH=\$PATH:/usr/pgsql-${MAJORVER}/bin" >  /etc/profile.d/postgres.sh
# source /etc/profile in bashrc of user postgres, make sure that PGDATA is defined and also PGVER so that we 
# can use PGVER in later scripts
echo "[ -f /etc/profile ] && source /etc/profile" >> /home/postgres/.bashrc 
echo "export PGDATA=/u01/pg${MAJORVER}/data" >> /home/postgres/.bashrc
echo "export PGVER=${MAJORVER}" >> /home/postgres/.bashrc
```

We need a systemd unit file for postgres, so that it is started automatically when we boot the server (see <https://www.postgresql.org/docs/11/server-start.html>)

```bash
cat <<EOF > /etc/systemd/system/postgresql.service
[Unit]
Description=PostgreSQL database server
Documentation=man:postgres(1)

[Service]
Type=notify
User=postgres
ExecStart=/usr/pgsql-11/bin/postgres -D /u01/pg11/data
ExecReload=/bin/kill -HUP $MAINPID
KillMode=mixed
KillSignal=SIGINT
TimeoutSec=0

[Install]
WantedBy=multi-user.target
EOF
```
enable the unit but don't start it yet

```
systemctl enable postgresql
```
Now we can init the database, as user postgres

```
su - postgres
# check that PGDATA and the PATH are correct
echo $PGDATA
echo $PATH
pg_ctl -D ${PGDATA} initdb -o "--auth=trust --encoding=UTF8 --locale='en_US.UTF8'"
```
to configure postgres, I prefer to keep the default and then include an additional file from the config.d directory: so at the end of the default postgresql.cond, I add the line "include_dir='conf.d'" and then I add custom configurations in conf.d
```
# as user postgres
mkdir $PGDATA/conf.d
echo "include_dir = 'conf.d'" >> $PGDATA/postgresql.conf
# now let's add some config
cat <<EOF > $PGDATA/conf.d/custom.conf
log_destination = 'syslog,csvlog'
logging_collector = on
# better to put the logs outside PGDATA so they are not included in the base_backup
log_directory = '/var/log/postgres'
log_filename = 'postgresql-%Y-%m-%d.log'
log_truncate_on_rotation = on
log_rotation_age = 1d
log_rotation_size = 0
# These are relevant when logging to syslog (if wanted, change log_destination to 'csvlog,syslog')
log_min_duration_statement=-1
log_duration = on
log_line_prefix='%m %c %u - ' 
log_statement = 'all'
log_connections = on
log_disconnections = on
log_checkpoints = on
log_timezone = 'Europe/Brussels'
# up to 30% of RAM. Too high is not good.
shared_buffers = 512MB
#checkpoint at least every 15min
checkpoint_timeout = 15min 
#if possible, be more restrictive
listen_addresses='*'
#for standby
max_replication_slots = 5
archive_mode = on
archive_command = '/opt/postgres/scripts/archive.sh  %p %f'
# archive_command = '/bin/true'
wal_level = replica
max_wal_senders = 5
hot_standby = on
hot_standby_feedback = on
# for pg_rewind
wal_log_hints=true
EOF
```
I prefer to put the logs outside PGDATA, otherwise a backup might become very big just because of the logs. So we must create the directory

```bash
 sudo mkdir /var/log/postgres
 sudo chown postgres:postgres /var/log/postgres
```

As you can see I use the script /opt/postgres/scripts/archive.sh as archive command, this needs to be created

```bash
sudo mkdir -p /opt/postgres/scripts
sudo chown -R postgres:postgres /opt/postgres
```

```bash
# content of script /opt/postgres/scripts/archive.sh
LOGFILE=/var/log/postgres/archive.log
if [ ! -f $LOGFILE ] ; then
 touch $LOGFILE
fi
echo "archiving $1 to /u02/archive/$2"
cp $1 /u02/archive/$2
exit $?
```

Now let's create a database and a user
```bash
sudo systemctl start posgres

psql -U postgres postgres
```
at the psql prompt, create a database (I use critlib), then quit
```
create database critlib encoding 'UTF8'  LC_COLLATE='en_US.UTF8';
\q
```
now reconnect to the newly create database and create two users (critlib_owner and critlib_user)
```
psql -U postgres critlib
create user critlib_owner nosuperuser nocreatedb login password 'critlib_owner';
create schema critlib_owner authorization critlib_owner;
create user critlib_user nosuperuser nocreatedb login password 'critlib_user';
grant usage on schema critlib_owner to critlib_user;
alter default privileges in schema critlib_owner grant select,insert,update,delete on tables to critlib_user;
alter role critlib_user set search_path to "$user",critlib_owner,public;
```

Before cloning the server (we need 3 servers) I will repmgr and pgpool. After that we can clone the server (it is a VM !), set-up the streaming replication with repmgr and then configure pgpool

## install repmgr

Repmgr is a nice open-source tool made by 2ndquadrant. It maybe worth to first set-up the streaming replication without using repmgr, just using standard postgres commands, for learning purpose. But apart from it is really worth using repmgr because it brings very well tested and documented scripts. Note that repmgr can also be used to automate the failover (via the repmgrd daemon process) but we don't want that because we will use pgpool

```bash
MAJORVER=11
REPMGRVER=4.2
curl https://dl.2ndquadrant.com/default/release/get/${MAJORVER}/rpm | bash
yum install -y --enablerepo=2ndquadrant-dl-default-release-pg${MAJORVER} --disablerepo=pgdg${MAJORVER} repmgr${MAJORVER}-${REPMGRVER}
mkdir /var/log/repmgr && chown postgres:postgres /var/log/repmgr
```

## install pgpool

Adding pgpool on Centos is easy: install the pgpool release rpm (it will set-up a repo file in /etc/yum.repo.d) and then install pgpool itself via yum. 

```bash
export PGPOOLMAJOR=4.0
export PGPOOLVER=4.0.3
export PGVER=11

yum install -y http://www.pgpool.net/yum/rpms/${PGPOOLMAJOR}/redhat/rhel-7-x86_64/pgpool-II-release-${PGPOOLMAJOR}-1.noarch.rpm
yum install --disablerepo=pgdg11 --enablerepo=pgpool40 -y pgpool-II-pg11-${PGPOOLVER} pgpool-II-pg11-extensions-${PGPOOLVER} pgpool-II-pg11-debuginfo-${PGPOOLVER}
``

I prefer to have pgpool running as user postgres, so I will override the systemd unit file that was installed by the rpm

```bash
# as user root
cat <<EOF > /etc/systemd/system/pgpool.service.d/override.conf
[Service]
User=postgres
Group=postgres
EOF
```

Since pgpool connects to the various postgres servers via ssh, I add a few config to the ssh client to make ssh connections easier
```bash
cat <<EOF > /etc/ssh/ssh_config
StrictHostKeyChecking no 
UserKnownHostsFile /dev/null 
EOF
```
Since I will use postgres user to start pgpool, I will change the ownership of /opt/pgpool-II 

```bash
sudo chown postgres:postgres -R /etc/pgpool-II
```

I also need to create a directory for the pid file and the socket file
```
sudo mkdir /var/run/pgpool
sudo chown postgres:postgres /var/run/pgpool
```

## Clone the servers

Now that I have pg01 ready, I will clone it to pg02 and pg03

I am using virsh (KVM) to manage my guest, those are the commands I used to clone my VM (the VM was created with this procedure <http://saule1508.github.io/libvirt-centos-cloud-image>)

Note: this is mostly for my own record, your way of cloning a VM might be different of course

```bash
virsh shutdown pg01
sudo mkdir /u01/virt/{pg02,pg03}
sudo chown pierre:pierre /u01/virt/pg02 /u01/virt/pg03
virt-clone -o pg01 -n pg02 --file /u01/virt/pg02/pg02.qcow2 --file /u01/virt/pg02/pg02-disk1.qcow2
```
Now start the VM, get into it and change its IP address. To have a DNS, I also add an entry in /etc/hosts of the host 
```
# /etc/hosts of the server, add one entry per vm so that they can speak to each other via dns
# added
192.168.122.10 pg01.localnet
192.168.122.11 pg02.localnet
192.168.122.13 pg03.localnet
```
when doing so, one must stop and start the default network then restart libvirtd

```bash
virsh net-destroy default
virsh net-start default
sudo systemctl restart libvirtd
```

# Set-up streaming replication

Now I have 3 centos 7 VM with postgres, repmgr and pgpool installed. The 3 VM can resolve each other names via DNS but alternatively we could put 3 entries in the /etc/hosts file on each VM
| host   | IP   |   |
|---|---|---|
| pg01.localnet  | 192.168.122.10  | 
| pg02.localnet  | 192.168.122.11  | 
| pg03.localnet  | 192.168.122.12  | 

We will use repmgr to set-up the 3 postgres instances in a primary - standby configuration where pg01 will be the primary. pg01 will be streaming his write-ahead-log (wal) to pg02 and pg03.

First we want to set-up ssh keys so that each servers can connect to each other with the user postgres. On each server, generate a ssh keys pairt for user postgres and transfer the public key to both other servers

```bash 
# on pg01 as user postgres. Keep the default (no passphrase)
ss-keygen -t rsa
ssh-copy-id postgres@pg02.localnet
ssh-copy-id postgres@pg03.localnet
```
do the same on server pg02 and on server pg03

For the streaming replication we will be using the user repmgr with password rep123

```bash
# on pg01
echo "*:*:repmgr:repmgr:rep123" > /home/postgres/.pgpass
echo "*:*:replication:repmgr:rep123" >> /home/postgres/.pgpass
chmod 600 /home/postgres/.pgpass
scp /home/postgres/.pgpass pg02.localnet:/home/postgres/.pgpass 
scp /home/postgres/.pgpass pg03.localnet:/home/postgres/.pgpass 
```

Add entries in $PGDATA/pg_hba.conf for repmgr
```bash
# on pg01
cat <<EOF >> $PGDATA/pg_hba.conf
# replication manager
local  replication   repmgr                      trust
host   replication   repmgr      127.0.0.1/32    trust
host   replication   repmgr      0.0.0.0/0       md5
local   repmgr        repmgr                     trust
host    repmgr        repmgr      127.0.0.1/32   trust
host    repmgr        repmgr      0.0.0.0/0      md5
host    all           all         0.0.0.0/0      md5
EOF

create the user repmgr
```bash
# on pg01
psql <<-EOF
  create user repmgr with superuser login password 'rep123' ;
  alter user repmgr set search_path to repmgr,"\$user",public;
  \q
EOF


create the repmgr configuration file
```bash
# on pg01
PGVER=11
cat <<EOF > /etc/repmgr/11/repmgr.conf
node_id=1
node_name=pg01.localnet
conninfo='host=pg01.localnet dbname=repmgr user=repmgr password=rep123 connect_timeout=2'
data_directory='/u01/pg11/data'
use_replication_slots=yes
#event_notification_command='/opt/postgres/scripts/repmgrd_event.sh %n "%e" %s "%t" "%d" %p %c %a'
reconnect_attempts=10
reconnect_interval=1

restore_command = 'cp /u02/archive/%f %p'

log_facility=STDERR
failover=no
monitor_interval_secs=5

pg_bindir='/usr/pgsql-11/bin'

service_start_command = 'sudo systemctl start postgres'
service_stop_command = 'sudo systemctl stop postgres'
service_restart_command = 'sudo systemctl restart postgres'
service_reload_command = 'pg_ctl reload'

promote_command='repmgr -f /etc/repmgr/11/repmgr.conf standby promote'
follow_command='repmgr -f /etc/repmgr/11/repmgr.conf standby follow -W --upstream-node-id=%n'
EOF




## Configure pgpool

The following line muse be added to the postgres config (not sure if it is still needed)

```bash
echo "pgpool.pg_ctl='/usr/pgsql-11/bin/pg_ctl'" >> $PGDATA/conf.d/custom.conf
```
similar to the postgres host based authentification mechanism (pg_hba), pgpool has a pool_hba.conf file. 

```bash
echo <<EOF >> /etc/pgpool-II/pool_hba.conf
local   all         all                               trust
# IPv4 local connections:
host     all         all         0.0.0.0/0             md5
EOF
```
Because I use md5, I will need to have the file pool_passwd containing the md5 hashed password <http://www.pgpool.net/docs/latest/en/html/runtime-config-connection.html#GUC-POOL-PASSWD>

```bash
# first dump the info into a temp file
psql -c "select rolname,rolpassword from pg_authid;" > /tmp/users.tmp
# then go through the file to remove/add the entry in pool_passwd file
cat /tmp/users.tmp | awk 'BEGIN {FS="|"}{print $1" "$2}' | grep md5 | while read f1 f2
do
 echo "setting passwd of $f1 in /etc/pgpool-II/pool_passwd"
 # delete the line if exits
 sed -i -e "/^${f1}:/d" /etc/pgpool-II/pool_passwd
 echo $f1:$f2 >> /etc/pgpool-II/pool_passwd
done
```

### Setting up pcp.conf file

pcp (pgpool control protocol) is an administrative interface to pgpool, it enables you to interact with pgpool via port 9898 (default). The file pcp.conf stores a username and a md5 password to authentificate, the file .pcppass enables a user to use pcp commands without password. Note that this user/password is not related to a postgres user, it is just a pcp user that can speak with pgpool via the pcp protocol. <http://www.pgpool.net/docs/latest/en/html/configuring-pcp-conf.html>

So I will use postgres with password secret

```bash
echo "postgres:$(pg_md5 secret)" >> /etc/pgpool-II/pcp.conf
echo "*:*:postgres:$(pg_md5 secret)" > /home/postgres/.pcppass 
chown postgres:postgres /home/postgres/.pcppass 
chmod 600 /home/postgres/.pcppass
```

In watchdog mode, pgpool will need to execute the ip and the arping commands with user postgres, so we set the sticky bit on those two utilities.

```bash
chmod 4755 /usr/sbin/ip /usr/sbin/arping
```

### Pgpool configuration

I start from a sample file, /etc/pgpool-II/pgpool.conf.sample-stream, then I will override some value by adding them to the end of the file

Since the end goal is to have 3 servers, with 3 instances of pgpool in watchdog mode and 3 instances of postgres in streaming replication, the config below references all 3 servers. I will explain them later once all 3 servers have been prepared.


```bash
CONFIG_FILE=/etc/pgpool-II/pgpool.conf
cp /etc/pgpool-II/pgpool.conf.sample-stream $CONFIG_FILE
cat <<EOF >> $CONFIG_FILE
# override section

listen_addresses = '*'
port = 9999
socket_dir = '/var/run/pgpool'
pcp_listen_addresses = '*'
pcp_port = 9898
pcp_socket_dir = '/var/run/pgpool'
listen_backlog_multiplier = 2
serialize_accept = off
enable_pool_hba = on
pool_passwd = 'pool_passwd'
authentication_timeout = 60
ssl = off
num_init_children = 10
max_pool = 5
# - Life time -
child_life_time = 300
child_max_connections = 0
connection_life_time = 600
client_idle_limit = 0

syslog_facility = 'LOCAL0'
syslog_ident = 'pgpool'
debug_level = 0

pid_file_name = '/var/run/pgpool/pgpool.pid'
logdir = '/tmp'

connection_cache = on
reset_query_list = 'ABORT; DISCARD ALL'
#reset_query_list = 'ABORT; RESET ALL; SET SESSION AUTHORIZATION DEFAULT'

replication_mode = off
load_balance_mode = off
master_slave_mode = on
master_slave_sub_mode = 'stream'

# this is about checking the streaming replication, i.e. what is the delay
sr_check_period = 10
sr_check_user = 'repmgr'
sr_check_password = 'repmgr'
sr_check_database = 'repmgr'
delay_threshold = 10000000

# this is about automatic failover
failover_command = '/opt/pgpool/scripts/failover.sh  %d %h %P %m %H %R'
failback_command = 'echo failback %d %h %p %D %m %H %M %P'
failover_on_backend_error = 'off'
search_primary_node_timeout = 300
follow_master_command = '/opt/pgpool/scripts/follow_master.sh %d %h %m %p %H %M %P'

health_check_period = 40
health_check_timeout = 10
health_check_user = 'hcuser'
health_check_password = 'hcuser'
health_check_database = 'postgres'
health_check_max_retries = 3
health_check_retry_delay = 1
connect_timeout = 10000

#------------------------------------------------------------------------------
# ONLINE RECOVERY
#------------------------------------------------------------------------------
recovery_user = 'postgres'
recovery_password = 'postgres'
recovery_1st_stage_command = 'pgpool_recovery.sh'
recovery_2nd_stage_command = 'echo recovery_2nd_stage_command'
recovery_timeout = 90
client_idle_limit_in_recovery = 0

#------------------------------------------------------------------------------
# WATCHDOG
#------------------------------------------------------------------------------
use_watchdog = on
# trusted_servers = 'www.google.com,pg02,pg03' (not needed with a 3 nodes cluster)
ping_path = '/bin'
wd_hostname = pg01
wd_port = 9000
wd_priority = 1
wd_authkey = ''
wd_ipc_socket_dir = '/var/run/pgpool'
delegate_IP = 192.168.122.99
if_cmd_path = '/opt/pgpool/scripts'
if_up_cmd = 'ip_w.sh addr add $_IP_$/24 dev eth0 label eth0:0'
if_down_cmd = 'ip_w.sh addr del $_IP_$/24 dev eth0'
arping_path = '/opt/pgpool/scripts'
arping_cmd = 'arping_w.sh -U $_IP_$ -I eth0 -w 1'
# - Behaivor on escalation Setting -

heartbeat_destination0 = 'pg01'
heartbeat_destination_port0 = 9694
heartbeat_destination1 = 'pg02'
heartbeat_destination_port1 = 9694
heartbeat_destination2 = 'pg03'
heartbeat_destination_port2 = 9694

other_pgpool_hostname0 = 'pg02'
other_pgpool_port0 = 9999
other_wd_port0 = 9000

other_pgpool_hostname1 = 'pg03'
other_pgpool_port1 = 9999
other_wd_port1 = 9000
EOF

## install repmgr


cat <<-EOF >> $PGDATA/pg_hba.conf
# replication manager
local  replication   repmgr                      trust
host   replication   repmgr      127.0.0.1/32    trust
host   replication   repmgr      0.0.0.0/0       md5
local   repmgr        repmgr                     trust
host    repmgr        repmgr      127.0.0.1/32   trust
host    repmgr        repmgr      0.0.0.0/0      md5
EOF
echo "host     all           all        0.0.0.0/0            md5" >> $PGDATA/pg_hba.conf

```




I will document step by step how to create a Centos guest from a cloud image, all at the command line so that the guest creation is very fast and easy to automate. My guest will be called pg01 (I will use it for postgres), of course change occurences of pg01 in this document to what suits you.

Some additional documentation:
* <https://cloudinit.readthedocs.io/en/latest/topics/examples.html>
* [red-hat clout_init doc](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/installation_and_configuration_guide/setting_up_cloud_init)
* <https://packetpushers.net/cloud-init-demystified> (not up-to-date but explains the concept well)
* <https://www.cyberciti.biz/faq/create-vm-using-the-qcow2-image-file-in-kvm>


## Install KVM and libvirt, config user

libvirt is normally installed by default on Fedora and on Centos 7, if not check for example <https://www.linuxtechi.com/install-kvm-hypervisor-on-centos-7-and-rhel-7/>

You don't need to use root to create guest VM's or to use virsh, you can keep your own user but make sure to add yourself in the libvirt group and check the env variable LIBVIRT_DEFAULT_URI

* add user to group libvirt

In my case I am using user critlib, so:
```
sudo usermod -G libvirt -a critlib
```
nb: this requires login on again

* add environment variable LIBVIRT_DEFAULT_URI

```bash
export LIBVIRT_DEFAULT_URI=qemu:///system
```
and add it to your profile also
```bash
echo export LIBVIRT_DEFAULT_URI=qemu:///system >> ~/.bash_profile
```

## Download the cloud image. 

This is fast because the image is small (895M).

```bash
# create a directory to store the downloaded image, mine is /data1/downloads
cd /data1/downloads 
wget https://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2
```

We can get info about the image
```bash
qemu-img info CentOS-7-x86_64-GenericCloud.qcow2
```
As you can see the image virtual size is 8.0G but the disk size is only 895M
```
image: CentOS-7-x86_64-GenericCloud.qcow2
file format: qcow2
virtual size: 8.0G (8589934592 bytes)
disk size: 895M
cluster_size: 65536
Format specific information:
    compat: 0.10
```
I prefer to resize the image (the space will not be allocated until it is used) so that I get a bigger root partition in my new guest.

```
qemu-img resize CentOS-7-x86_64-GenericCloud.qcow2 20G
```
If we look again the info, the virtual size is 20G but the file size did not change (895M)

Next I create a storage pool for the new guest (just a directory on the host). I use an env variable to point to the directory in the remainder of this text. Change pg01 by whatever you decided for your new VM name.

```bash
export VMPOOLDIR=/data2/virtpool/pg01
sudo mkdir $VMPOOLDIR
sudo chown critlib:critlib $VMPOOLDIR
virsh pool-create-as --name pg01 --type dir --target $VMPOOLDIR
```

## Prepare the iso for cloud-init

we need to create two files called user-data and meta-data, those two files will be put in the iso. Again change pg01 as suits you.

Since I like to have static IP for my guest, I included the section network-interfaces in the meta-data file. If you don't need a fixed IP (or if you prefer to configure it after), remove this section and a few line from the file user-data as well (see below) so that an IP will be assigned via dhcp by libvirt. 

```bash
cat > $VMPOOLDIR/meta-data <<EOF
instance-id: pg01
local-hostname: pg01
network-interfaces: |
  iface eth0 inet static
  address 192.168.122.10
  network 192.168.122.0
  netmask 255.255.255.0
  broadcast 192.168.122.255
  gateway 192.168.122.1
EOF
```
The second file is user-data. In this file I will reference my public key, so make sure to have a ssh keys pair in the directory HOME/.ssh. You can generate a key pair with ssh-keygen.

```
ssh-keygen -t rsa
```

the public key in $HOME/.ssh/id_rsa.pub will be injected in the guest, so that you will be able to log on from the host via ssh. 

To have a static IP configured, I added some hacking in the runcmd section in the user-data file. Maybe there is a way to express that I want NM_CONTROLLED to no and ONBOOT to yes via the network section in the meta-data file above ? But it did not work. The commands ifdown/ifup in the runcmd section come from red-hat documentation (workaround for a bug ?). If you don't need a static IP, remove the network-interfaces section from meta-data and remove lines from section runcmd below (all lines except the first, i.e. keep the remove of cloud-init), remove also the DNS and resolv_conf settings.

Of course adapt the hostname to your need, in this example it is pg01

About the *chpasswd* section, it will set a password for root. This is super handy in case of problems because you can log through the console. But take care that the keyboard for the console might be qwerty and not azerty on first boot...

```bash
cat > $VMPOOLDIR/user-data <<EOF
#cloud-config
# Hostname management
preserve_hostname: False
hostname: pg01
fqdn: pg01.localnet
# Setup Users with ssh keys so that I can log in into new machine
users:
  - default
  - name: ansible
    groups: ['wheel']
    shell: /bin/bash
    sudo: ALL=(ALL) NOPASSWD:ALL
    ssh-authorized-keys:
      - $(cat $HOME/.ssh/id_rsa.pub)
cloud_config_modules: 
  - resolv_conf
# set timezone for VM
timezone: Europe/Brussels
# Remove cloud-init when finished with it
# some patches of network config that I could not do via network section in meta-data
runcmd:
  - [ yum, -y, remove, cloud-init ]
  - [ ifdown, eth0 ]
  - [ ifup, eth0 ]
  - [ sed, -i, -e, "s/ONBOOT=no/ONBOOT=yes/", /etc/sysconfig/network-scripts/ifcfg-eth0 ]
  - [ sed, -i, -e, "\$aNM_CONTROLLED=no", /etc/sysconfig/network-scripts/ifcfg-eth0 ]
# Set DNS
manage_resolv_conf: true
# we'll use dnsmask on the host
resolv_conf:
  nameservers: ['192.168.122.1']

# Configure where output will go
output: 
  all: ">> /var/log/cloud-init.log"
# configure interaction with ssh server
ssh_deletekeys: True
ssh_genkeytypes: ['rsa', 'ecdsa']
# Install my public ssh key to the first user-defined user configured 
# in cloud.cfg in the template (which is centos for CentOS cloud images)
ssh_authorized_keys:
  - ssh-rsa $(cat $HOME/.ssh/id_rsa.pub)
# So that we can logon from the console should ssh not be available
chpasswd:
  list: |
    root:password
  expire: False  
EOF
```
Now with cloud-utils we can create a bootable iso. The help of this command says "Create a disk for cloud-init to utilize nocloud". Not sure what this means, but basically it uses user-data and meta-data to produce an iso with which we can boot our cloud image so that cloud-init will do its job at first boot.

```bash
sudo yum install cloud-utils mkisofs genisoimage
cd $VMPOOLDIR
cloud-localds pg01.iso user-data meta-data
```
On fedora, I have a strange error image "genisoimage is required". However dns install genisoimage will do nothing because mkisofs is already installed and supposedly mkisofs is a new name for genisoimage. To make it work I created a symlink from mkisofs to genisoimage

```bash
ln -s /usr/bin/mkisofs /usr/bin/genisoimage
```

With this iso and the cloud image we downloaded, we will be able to provision our new guest. 

## create a new guest

First copy the downloaded cloud image in the pool. I do that using qemu-img convert but a simple copy is OK
```bash
qemu-img convert -O qcow2 /data1/downloads/CentOS-7-x86_64-GenericCloud.qcow2 $VMPOOLDIR/pg01.qcow2
```

then run virt-install to create the VM

```bash
virt-install --name pg01 --memory 1024 --vcpus 2 --disk $VMPOOLDIR/pg01.qcow2,device=disk,bus=virtio --os-type generic --os-variant centos7.0 --virt-type kvm --network network=default,model=virtio --cdrom $VMPOOLDIR/pg01.iso 
```

If you do not specify --noautoconsole in the virt-install command, the program tries to start virt-viewer so that one can see the progress of the installation. When at the end, it prompts for a login, reboot the VM. If you have --noautoconsole then just wait long enough (a few minutes, it is very fast)

If you specified a static IP, then you know the IP otherwise you can get the IP of the new guest:

```bash
virsh domifaddr pg01
```

then I can ssh with user ansible (that I specified in the user-data file) or with user centos (defined in the cloud image). Since I copied my public key (via user-data file) I don't need a password. I strongly recommand to change the root password so that you can connect via the console later on if you are stuck with ssh (wrong network config for example)

We can get rid of the iso and user-data and meta-data
```
rm $VMPOOLDIR/pg01.iso $VMPOOLDIR/user-data $VMPOOLDIR/meta-data
```

What's left is:
* configure the VM to have a static IP (if not done via cloud-config)
* Add a disk and create a volume group

## Static IP

What I would do 
1. change the dhcp range of the default network (NB: the default network uses NAT, if you need to access the VM's from outside the host then you must create a bridge network)

```bash
virsh net-edit default
```
and change the dhcp range 
```
    <dhcp>
      <range start='192.168.122.100' end='192.168.122.254'/>
    </dhcp>
```
so that IP's between 192.168.122.2 and 192.168.122.99 are reserved for static allocation.

We can see that the change is not active
```
virsh net-dumpxml default 
virsh net-dumpxml default --inactive
```
We must stop and restart the default network
```
virsh net-destroy default
virsh net-start default
virsh net-dumpxml default 
```
then we must restart libvirtd
```bash
systemctl restart libvirtd
```
and we must reboot our VM then wait that the new IP is allocated
```
virsh reboot pg01
```

2. Change the network configuration of the VM.

If doing it via cloud-init did not work, I would just do it manually...

get into the VM as root, and edit the file /etc/sysconfig/network-scripts/ifcfg-eth0

```
BOOTPROTO=static
NM_CONTROLLED="no"
DEVICE=eth0
HWADDR=< keep existing one >
IPADDR=192.168.122.10
PREFIX=24
GATEWAY=192.168.122.1
DNS1=192.168.122.1
DOMAIN=localnet
ONBOOT=yes
TYPE=Ethernet
USERCTL=no
```


3. add vm in /etc/hosts on the host

With libvird on the host, there is a dnsmask server automatically started so that the content of the /etc/hosts on the host will be made available to the guests via DNS. This is great because VM's can connect to each other via DNS.

In /etc/hosts on the host 
```
192.168.122.10 pg01.localnet
```

## LVM 

first create a disk on the host

```bash
virsh vol-create-as --pool pg01 --name pg01-disk1.qcow2 --capacity 40G --allocation 10G --format qcow2
```
and allocate it to the VM
```bash
virsh attach-disk --domain pg01 --source $VMPOOLDIR/pg01-disk1.qcow2 --target vdb --persistent --driver qemu --subdriver qcow2
```
Then get into the VM and set-up the volume group, the logical volume and the file system
```bash
# as user root
yum install lvm2
pvcreate /dev/vdb
vgcreate vg01 /dev/vdb
lvcreate -l100%FREE -n lv_01 vg01
mkfs -t xfs /dev/vg01/lv_01
mkdir /u01
```

edit /etc/fstab and add
```
/dev/vg01/lv_01 /u01 xfs defaults 0 0
```

then mount the filesystem

```bash
mount /u01
```

The VM can be cloned, you can take snapshot, ... you can do pretty everything via the command line. I'll document that in a next post.
