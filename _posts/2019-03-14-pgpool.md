---
layout: post
title: Postgres automatic (and mostly transparent) failover with pgpool
published: false
---

I have been working a lot with pgpool recently and I wanted to document my experiments in case it is helpful for others. I deployed postgres and pgpool as docker images and I build a graphical interface to operate and vizualize the cluster. This is all in the github repo <https://github.com/saule1508/pgcluster>

However in this post I will rather install postgres/pgpool natively and explain the key pgpool concepts that I found difficult to grasp. This said, if you start a learning journey with pgpool and postgres, I find it makes a lot of sense to use docker as this makes experimenting much easier.

In this post I will document a 3 nodes clustes in which pgpool is made high-available via the watch-dog mode. This requires 3 servers. I am using Centos VM's provisioned automatically via libvirt, but you can use vagrant or provision the centos guests manually. If you have a linux host then I suggest to use libvirt, I documented here a way to provision a centos guest from the command line in a few minutes: [http://saule1508.github.io/libvirt-centos-cloud-image/](Centos guest VM with cloud-init), this is just great.

## Step 1: provision 3 guests machines and install postgres

Create a first Centos guest machine (use the link above if you have libvirt)

I create a directory /u01 in which the database will reside. I also create a user postgres with uid 50010 (arbitrary id, does not matter but it is better to have the same id on all servers)

## Postgres


```bash
# as user root
MAJORVER=11
MINORVER=2
yum update -y
# install some useful packages
yum install -y epel-release libxslt sudo openssh-server openssh-clients jq passwd rsync iproute python-setuptools \
    hostname inotify-tools yum-utils which sudo vi
if [ ! -d /root/.ssh ] ; then
  ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key -N ''
fi
# create user postgres before installing postgres rpm, because I want to fix the uid
useradd -u 50010 postgres
# because I like that postgres can become root without password, I add it in sudoers 
echo "postgres ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/postgres
# install postgres release rpm, this will add a file /etc/yum.repos.d/pgdg-11-centos.repo
yum install -y https://download.postgresql.org/pub/repos/yum/${MAJORVER}/redhat/rhel-7-x86_64/pgdg-centos${MAJORVER}-${MAJORVER}-${MINORVER}.noarch.rpm
# with the new repo defined (pgdg11), ww can install postgres
# of course postgres is also available from Centos base repos, but we want the latest version (11)
yum install -y postgresql${MAJORVER}-${MAJORVER}.${MINORVER} postgresql${MAJORVER}-server-${MAJORVER}.${MINORVER}  postgresql${MAJORVER}-contrib-${MAJORVER}.${MINORVER}
# verify 
yum list installed postgresql*
```
I want to store the postgres database in /u01/pg11/data (non-default location), I want to have the archived wal in /u02/archive and the backup in /u02/backup

```bash
mkdir -p /u01/pg${MAJORVER}/data /u02/archive /u02/backup
chown postgres:postgres /u01/pg${MAJORVER}/data /u02/archive /u02/backup
chmod 700 /u01/pg${MAJORVER}/data /u02/archive
```
The environment variable PGDATA is critical: it points to the database location. We need to change it from the rpm default /var/lib/pgsql/11/data to the new location /u01/pg11/data.

```
export PGDATA=/u01/pg${MAJORVER}/data
# add the binaries in the path of all users
echo "export PATH=\$PATH:/usr/pgsql-${MAJORVER}/bin" >  /etc/profile.d/postgres.sh
# source /etc/profile in bashrc of user postgres, make sure that PGDATA is defined and also PGVER so that we 
# can use PGVER in later scripts
echo "[ -f /etc/profile ] && source /etc/profile" >> /home/postgres/.bashrc 
echo "export PGDATA=/u01/pg${MAJORVER}/data" >> /home/postgres/.bashrc
echo "export PGVER=${MAJORVER}" >> /home/postgres/.bashrc
```

We need a systemd unit file for postgres, so that it is started automatically when we boot the server (see <https://www.postgresql.org/docs/11/server-start.html>)

```bash
cat <<EOF > /etc/systemd/system/postgresql.service
[Unit]
Description=PostgreSQL database server
Documentation=man:postgres(1)

[Service]
Type=notify
User=postgres
ExecStart=/usr/pgsql-11/bin/postgres -D /u01/pg11/data
ExecReload=/bin/kill -HUP $MAINPID
KillMode=mixed
KillSignal=SIGINT
TimeoutSec=0

[Install]
WantedBy=multi-user.target
EOF
```
enable the unit but don't start it yet

```
systemctl enable postgresql
```
Now we can init the database, as user postgres

```
su - postgres
# check that PGDATA and the PATH are correct
echo $PGDATA
echo $PATH
pg_ctl -D ${PGDATA} initdb -o "--auth=trust --encoding=UTF8 --locale='en_US.UTF8'"
```
to configure postgres, I prefer to keep the default and then include an additional file from the config.d directory
```
# as user postgres
mkdir $PGDATA/conf.d
echo "include_dir = 'conf.d'" >> $PGDATA/postgresql.conf
# now let's add some config
cat <<EOF > $PGDATA/conf.d/custom.conf
log_destination = 'syslog,csvlog'
logging_collector = on
# better to put the logs outside PGDATA so they are not included in the base_backup
log_directory = '/var/log/postgres'
log_filename = 'postgresql-%Y-%m-%d.log'
log_truncate_on_rotation = on
log_rotation_age = 1d
log_rotation_size = 0
# These are relevant when logging to syslog (if wanted, change log_destination to 'csvlog,syslog')
log_min_duration_statement=-1
log_duration = on
log_line_prefix='%m %c %u - ' 
log_statement = 'all'
log_connections = on
log_disconnections = on
log_checkpoints = on
log_timezone = 'Europe/Brussels'
# up to 30% of RAM. Too high is not good.
shared_buffers = 512MB
#checkpoint at least every 15min
checkpoint_timeout = 15min 
#if possible, be more restrictive
listen_addresses='*'
#for standby
max_replication_slots = 5
archive_mode = on
archive_command = '/opt/posgres/scripts/archive.sh  %p %f'
# archive_command = '/bin/true'
wal_level = replica
max_wal_senders = 5
hot_standby = on
hot_standby_feedback = on
# for pg_rewind
wal_log_hints=true
EOF
```
I prefer to put the logs outside PGDATA, otherwise a backup might become very big just because of the logs. So we must create the directory

```bash
 sudo mkdir /var/log/postgres
 sudo chown postgres:postgres /var/log/postgres
```

As you can see I use the script /opt/postgres/scripts/archive.sh as archive command, this needs to be created

```bash
sudo mkdir -p /opt/postgres/scripts
sudo chown -R postgres:postgres /opt/postgres
```

```bash
# content of script /opt/postgres/scripts/archive.sh
cat <<EOF > /opt/postgres/scripts/archive.sh
LOGFILE=/var/log/postgres/archive.log
if [ ! -f $LOGFILE ] ; then
 touch $LOGFILE
fi
echo "archiving $1 to /u02/archive/$2"
cp $1 /u02/archive/$2
exit $?
EOF
chmod +x /opt/postgres/scripts/archive.sh
```

Now let's create a database and a user
```bash
sudo systemctl start posgresql
# verify
sudo systemctl status posgresql


psql -U postgres postgres
```
at the psql prompt, create a database (my database is called critlib), then quit
```
create database critlib encoding 'UTF8'  LC_COLLATE='en_US.UTF8';
\q
```
now reconnect to the newly create database and create two users (cl_owner and cl_user)
```
psql -U postgres critlib
create user cl_owner nosuperuser nocreatedb login password 'cl_owner';
create schema cl_owner authorization cl_owner;
create user cl_user nosuperuser nocreatedb login password 'cl_user';
grant usage on schema cl_owner to cl_user;
alter default privileges in schema cl_owner grant select,insert,update,delete on tables to cl_user;
alter role cl_user set search_path to "$user",cl_owner,public;
\q
```

## PGPOOL

Add in postgres config ?

pgpool.pg_ctl='/usr/pgsql-11/bin/pg_ctl'


# install pgpool release rpm
yum install -y http://www.pgpool.net/yum/rpms/${PGPOOLMAJOR}/redhat/rhel-7-x86_64/pgpool-II-release-${PGPOOLMAJOR}-1.noarch.rpm
# install pgpool
yum install --disablerepo=pgdg${MAJORVER} --enablerepo=pgpool40 -y \
  pgpool-II-pg${MAJORVER}-${PGPOOLVER} pgpool-II-pg${MAJORVER}-extensions-${PGPOOLVER} pgpool-II-pg${MAJORVER}-debuginfo-${PGPOOLVER}
# repmgr repo
curl https://dl.2ndquadrant.com/default/release/get/${MAJORVER}/rpm | bash
# install repmgr
yum install -y --enablerepo=2ndquadrant-dl-default-release-pg${MAJORVER} --disablerepo=pgdg${MAJORVER} repmgr${MAJORVER}-${REPMGRVER}
mkdir /var/log/repmgr /var/log/postgres
chown postgres:postgres /var/log/repmgr /var/log/postgres
if [ ! -d /u01 ] ; then
  mkdir /u01
fi
mkdir $PGDATA/conf.d
echo "include_dir = 'conf.d'" >> $PGDATA/postgresql.conf
cat <<-EOF >> $PGDATA/pg_hba.conf
# replication manager
local  replication   repmgr                      trust
host   replication   repmgr      127.0.0.1/32    trust
host   replication   repmgr      0.0.0.0/0       md5
local   repmgr        repmgr                     trust
host    repmgr        repmgr      127.0.0.1/32   trust
host    repmgr        repmgr      0.0.0.0/0      md5
EOF
echo "host     all           all        0.0.0.0/0            md5" >> $PGDATA/pg_hba.conf

```




I will document step by step how to create a Centos guest from a cloud image, all at the command line so that the guest creation is very fast and easy to automate. My guest will be called pg01 (I will use it for postgres), of course change occurences of pg01 in this document to what suits you.

Some additional documentation:
* <https://cloudinit.readthedocs.io/en/latest/topics/examples.html>
* [red-hat clout_init doc](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/installation_and_configuration_guide/setting_up_cloud_init)
* <https://packetpushers.net/cloud-init-demystified> (not up-to-date but explains the concept well)
* <https://www.cyberciti.biz/faq/create-vm-using-the-qcow2-image-file-in-kvm>


## Install KVM and libvirt, config user

libvirt is normally installed by default on Fedora and on Centos 7, if not check for example <https://www.linuxtechi.com/install-kvm-hypervisor-on-centos-7-and-rhel-7/>

You don't need to use root to create guest VM's or to use virsh, you can keep your own user but make sure to add yourself in the libvirt group and check the env variable LIBVIRT_DEFAULT_URI

* add user to group libvirt

In my case I am using user cl, so:
```
sudo usermod -G libvirt -a cl
```
nb: this requires login on again

* add environment variable LIBVIRT_DEFAULT_URI

```bash
export LIBVIRT_DEFAULT_URI=qemu:///system
```
and add it to your profile also
```bash
echo export LIBVIRT_DEFAULT_URI=qemu:///system >> ~/.bash_profile
```

## Download the cloud image. 

This is fast because the image is small (895M).

```bash
# create a directory to store the downloaded image, mine is /data1/downloads
cd /data1/downloads 
wget https://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2
```

We can get info about the image
```bash
qemu-img info CentOS-7-x86_64-GenericCloud.qcow2
```
As you can see the image virtual size is 8.0G but the disk size is only 895M
```
image: CentOS-7-x86_64-GenericCloud.qcow2
file format: qcow2
virtual size: 8.0G (8589934592 bytes)
disk size: 895M
cluster_size: 65536
Format specific information:
    compat: 0.10
```
I prefer to resize the image (the space will not be allocated until it is used) so that I get a bigger root partition in my new guest.

```
qemu-img resize CentOS-7-x86_64-GenericCloud.qcow2 20G
```
If we look again the info, the virtual size is 20G but the file size did not change (895M)

Next I create a storage pool for the new guest (just a directory on the host). I use an env variable to point to the directory in the remainder of this text. Change pg01 by whatever you decided for your new VM name.

```bash
export VMPOOLDIR=/data2/virtpool/pg01
sudo mkdir $VMPOOLDIR
sudo chown cl:cl $VMPOOLDIR
virsh pool-create-as --name pg01 --type dir --target $VMPOOLDIR
```

## Prepare the iso for cloud-init

we need to create two files called user-data and meta-data, those two files will be put in the iso. Again change pg01 as suits you.

Since I like to have static IP for my guest, I included the section network-interfaces in the meta-data file. If you don't need a fixed IP (or if you prefer to configure it after), remove this section and a few line from the file user-data as well (see below) so that an IP will be assigned via dhcp by libvirt. 

```bash
cat > $VMPOOLDIR/meta-data <<EOF
instance-id: pg01
local-hostname: pg01
network-interfaces: |
  iface eth0 inet static
  address 192.168.122.10
  network 192.168.122.0
  netmask 255.255.255.0
  broadcast 192.168.122.255
  gateway 192.168.122.1
EOF
```
The second file is user-data. In this file I will reference my public key, so make sure to have a ssh keys pair in the directory HOME/.ssh. You can generate a key pair with ssh-keygen.

```
ssh-keygen -t rsa
```

the public key in $HOME/.ssh/id_rsa.pub will be injected in the guest, so that you will be able to log on from the host via ssh. 

To have a static IP configured, I added some hacking in the runcmd section in the user-data file. Maybe there is a way to express that I want NM_CONTROLLED to no and ONBOOT to yes via the network section in the meta-data file above ? But it did not work. The commands ifdown/ifup in the runcmd section come from red-hat documentation (workaround for a bug ?). If you don't need a static IP, remove the network-interfaces section from meta-data and remove lines from section runcmd below (all lines except the first, i.e. keep the remove of cloud-init), remove also the DNS and resolv_conf settings.

Of course adapt the hostname to your need, in this example it is pg01

About the *chpasswd* section, it will set a password for root. This is super handy in case of problems because you can log through the console. But take care that the keyboard for the console might be qwerty and not azerty on first boot...

```bash
cat > $VMPOOLDIR/user-data <<EOF
#cloud-config
# Hostname management
preserve_hostname: False
hostname: pg01
fqdn: pg01.localnet
# Setup Users with ssh keys so that I can log in into new machine
users:
  - default
  - name: ansible
    groups: ['wheel']
    shell: /bin/bash
    sudo: ALL=(ALL) NOPASSWD:ALL
    ssh-authorized-keys:
      - $(cat $HOME/.ssh/id_rsa.pub)
cloud_config_modules: 
  - resolv_conf
# set timezone for VM
timezone: Europe/Brussels
# Remove cloud-init when finished with it
# some patches of network config that I could not do via network section in meta-data
runcmd:
  - [ yum, -y, remove, cloud-init ]
  - [ ifdown, eth0 ]
  - [ ifup, eth0 ]
  - [ sed, -i, -e, "s/ONBOOT=no/ONBOOT=yes/", /etc/sysconfig/network-scripts/ifcfg-eth0 ]
  - [ sed, -i, -e, "\$aNM_CONTROLLED=no", /etc/sysconfig/network-scripts/ifcfg-eth0 ]
# Set DNS
manage_resolv_conf: true
# we'll use dnsmask on the host
resolv_conf:
  nameservers: ['192.168.122.1']

# Configure where output will go
output: 
  all: ">> /var/log/cloud-init.log"
# configure interaction with ssh server
ssh_deletekeys: True
ssh_genkeytypes: ['rsa', 'ecdsa']
# Install my public ssh key to the first user-defined user configured 
# in cloud.cfg in the template (which is centos for CentOS cloud images)
ssh_authorized_keys:
  - ssh-rsa $(cat $HOME/.ssh/id_rsa.pub)
# So that we can logon from the console should ssh not be available
chpasswd:
  list: |
    root:password
  expire: False  
EOF
```
Now with cloud-utils we can create a bootable iso. The help of this command says "Create a disk for cloud-init to utilize nocloud". Not sure what this means, but basically it uses user-data and meta-data to produce an iso with which we can boot our cloud image so that cloud-init will do its job at first boot.

```bash
sudo yum install cloud-utils mkisofs genisoimage
cd $VMPOOLDIR
cloud-localds pg01.iso user-data meta-data
```
On fedora, I have a strange error image "genisoimage is required". However dns install genisoimage will do nothing because mkisofs is already installed and supposedly mkisofs is a new name for genisoimage. To make it work I created a symlink from mkisofs to genisoimage

```bash
ln -s /usr/bin/mkisofs /usr/bin/genisoimage
```

With this iso and the cloud image we downloaded, we will be able to provision our new guest. 

## create a new guest

First copy the downloaded cloud image in the pool. I do that using qemu-img convert but a simple copy is OK
```bash
qemu-img convert -O qcow2 /data1/downloads/CentOS-7-x86_64-GenericCloud.qcow2 $VMPOOLDIR/pg01.qcow2
```

then run virt-install to create the VM

```bash
virt-install --name pg01 --memory 1024 --vcpus 2 --disk $VMPOOLDIR/pg01.qcow2,device=disk,bus=virtio --os-type generic --os-variant centos7.0 --virt-type kvm --network network=default,model=virtio --cdrom $VMPOOLDIR/pg01.iso 
```

If you do not specify --noautoconsole in the virt-install command, the program tries to start virt-viewer so that one can see the progress of the installation. When at the end, it prompts for a login, reboot the VM. If you have --noautoconsole then just wait long enough (a few minutes, it is very fast)

If you specified a static IP, then you know the IP otherwise you can get the IP of the new guest:

```bash
virsh domifaddr pg01
```

then I can ssh with user ansible (that I specified in the user-data file) or with user centos (defined in the cloud image). Since I copied my public key (via user-data file) I don't need a password. I strongly recommand to change the root password so that you can connect via the console later on if you are stuck with ssh (wrong network config for example)

We can get rid of the iso and user-data and meta-data
```
rm $VMPOOLDIR/pg01.iso $VMPOOLDIR/user-data $VMPOOLDIR/meta-data
```

What's left is:
* configure the VM to have a static IP (if not done via cloud-config)
* Add a disk and create a volume group

## Static IP

What I would do 
1. change the dhcp range of the default network (NB: the default network uses NAT, if you need to access the VM's from outside the host then you must create a bridge network)

```bash
virsh net-edit default
```
and change the dhcp range 
```
    <dhcp>
      <range start='192.168.122.100' end='192.168.122.254'/>
    </dhcp>
```
so that IP's between 192.168.122.2 and 192.168.122.99 are reserved for static allocation.

We can see that the change is not active
```
virsh net-dumpxml default 
virsh net-dumpxml default --inactive
```
We must stop and restart the default network
```
virsh net-destroy default
virsh net-start default
virsh net-dumpxml default 
```
then we must restart libvirtd
```bash
systemctl restart libvirtd
```
and we must reboot our VM then wait that the new IP is allocated
```
virsh reboot pg01
```

2. Change the network configuration of the VM.

If doing it via cloud-init did not work, I would just do it manually...

get into the VM as root, and edit the file /etc/sysconfig/network-scripts/ifcfg-eth0

```
BOOTPROTO=static
NM_CONTROLLED="no"
DEVICE=eth0
HWADDR=< keep existing one >
IPADDR=192.168.122.10
PREFIX=24
GATEWAY=192.168.122.1
DNS1=192.168.122.1
DOMAIN=localnet
ONBOOT=yes
TYPE=Ethernet
USERCTL=no
```


3. add vm in /etc/hosts on the host

With libvird on the host, there is a dnsmask server automatically started so that the content of the /etc/hosts on the host will be made available to the guests via DNS. This is great because VM's can connect to each other via DNS.

In /etc/hosts on the host 
```
192.168.122.10 pg01.localnet
```

## LVM 

first create a disk on the host

```bash
virsh vol-create-as --pool pg01 --name pg01-disk1.qcow2 --capacity 40G --allocation 10G --format qcow2
```
and allocate it to the VM
```bash
virsh attach-disk --domain pg01 --source $VMPOOLDIR/pg01-disk1.qcow2 --target vdb --persistent --driver qemu --subdriver qcow2
```
Then get into the VM and set-up the volume group, the logical volume and the file system
```bash
# as user root
yum install lvm2
pvcreate /dev/vdb
vgcreate vg01 /dev/vdb
lvcreate -l100%FREE -n lv_01 vg01
mkfs -t xfs /dev/vg01/lv_01
mkdir /u01
```

edit /etc/fstab and add
```
/dev/vg01/lv_01 /u01 xfs defaults 0 0
```

then mount the filesystem

```bash
mount /u01
```

The VM can be cloned, you can take snapshot, ... you can do pretty everything via the command line. I'll document that in a next post.
