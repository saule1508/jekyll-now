---
layout: post
title: Oracle HAIP
published: false
---

Some experiments with Oracle HAIP
<!--more-->

I have set-up a RAC on KVM on my home computer (Fedora). The two guest servers are installed with Centos 7 and Oracle/Grid 19. Initially I have only one interface for the interconnect and ASM communication. I added a second interface to build a HAIP (High availability IP) but then I faced database instance evicton. So - very important - **if you use an interface for private interconnect you must disable reverse path filtering for it**

Put this in /etc/sysctl.conf

```bash
net.ipv4.conf.eth1.rp_filter = 2
net.ipv4.conf.eth2.rp_filter = 2
```

then execute

```bash
sysctl -p
```

To verify

```bash
[root@ora01 orachk]# cat /proc/sys/net/ipv4/conf/eth1/rp_filter
2
[root@ora01 orachk]# cat /proc/sys/net/ipv4/conf/eth2/rp_filter
2
```

Link to the set-up of my home rac cluster: [Set-up Oracle RAC on libvirt](http://saule1508.github.io/oracle-RAC-libvirt)

## basic commands

```bash
oifcfg iflist -n -p
```

The -p flag implies that Oracle will make an assumption on the type of interface, it is only an assumption

```bash
oifcfg getif
```

```bash
oifcfg getif -if eth0/192.168.122.0
eth0  192.168.122.0  global  public

oifcfg getif -if eth1/10.0.0.0
eth1  10.0.0.0  global  cluster_interconnect,asm
```

ip addr shows that the HAIP is assigned to eth1 (as an alias)

```bash
ip addr show eth1
```

shows

```text
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:e0:5c:78 brd ff:ff:ff:ff:ff:ff
    inet 10.0.0.10/24 brd 10.0.0.255 scope global eth1
       valid_lft forever preferred_lft forever
    inet 169.254.30.155/19 brd 169.254.31.255 scope global eth1:1
       valid_lft forever preferred_lft forever
    inet6 fe80::5054:ff:fee0:5c78/64 scope link 
       valid_lft forever preferred_lft forever
```

On node 2, ip addr shows this IP

```text
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:2f:93:2b brd ff:ff:ff:ff:ff:ff
    inet 10.0.0.11/24 brd 10.0.0.255 scope global eth1
       valid_lft forever preferred_lft forever
    inet 169.254.7.4/19 brd 169.254.31.255 scope global eth1:1
       valid_lft forever preferred_lft forever
    inet6 fe80::5054:ff:fe2f:932b/64 scope link 
       valid_lft forever preferred_lft forever
```

check the resource:

```bash
crsctl status resource -t -init
```

```text
ora.cluster_interconnect.haip
      1        ONLINE  ONLINE       ora01                    STABLE
```

## Adding an interface

Now let's add a second interface for the interconnect. This implies adding the interface at OS level and then in clusterware. Adding the interface in clusterware is done via oifcfg setif, it requires a complete shutdown of crs.

First I need to add a virtual network in libvirt, I do this using the graphical interface virt-manager

In virtual manager, go to Edit -> Connection Details -> Virtual Network. I will add a second network, called rac_private_2

[Virt manager add second private network](../images/KVM_private_network_2.png)

Then I add a new interface to each of the guest, linked to this network. Copy the MAC ADDR assigned to the new interface, it will be needed to configure the interface at OS level.

[Virt manager add network interface to guest](../images/add_virtual_netif.png)

This requires a reboot of the guests

After reboot I can configure the newly added interface; add the file /etc/sysconfig/network-scripts/ifcfg-eth2

```text
BOOTPROTO=none
DEFROUTE=no
DEVICE=eth2
GATEWAY=192.168.100.1
IPADDR=192.168.100.128
NETMASK=255.255.255.0
ONBOOT=yes
HWADDR=52:54:00:e0:5c:78
TYPE=Ethernet
USERCTL=no
NM_CONTROLLED=no
```

on node 2 I use the IPADDR 192.168.100.129. Don't forget to adapt the HWADDR field

Start the interface

```bash
ip link set eth2 up
```

While crs is running we can set a new interface

```bash
oifcfg setif -global eth2/192.168.100.0:cluster_interconnect
```

In order to add or remove a private interface a complete stop/start of crs on both nodes is required, i.e. a rolling restart is not enough.

```bash
crsctl stop clusterware -all
```

start it on node 1

```bash
crsctl start crs
```

check the file ohasd_orarootagent_root for messages related to HAIP

check the resource

```bash
crsctl status resource ora.cluster_interconnect.haip -init
```

check the IP's, each private interface will have an HAIP assigned to it

```bash
ip addr
```

Start crs on node 2

```bash
crsctl start crs
```

check HAIP in the database

```sql
select * from gv$cluster_interconnects;
```